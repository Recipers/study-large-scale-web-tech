# 7. 알고리즘 실용화
- 가까운 예로 보는 이론·연구의 실전 투입
### 알고리즘·데이터 구조 선택의 중요성
#### 문제해결에 적합한 알고리즘 & 데이터 구조
- 속도를 중시하는 프로그램 작성 시 알고리즘과 데이터 구조의 선택은 중요
- 데이터가 클수록 차이가 현저해짐
## 7-19. 알고리즘과 평가
### 데이터 규모와 계산량 차이
- 선형탐색
    - n건에 대해 n번의 탐색이 필요 O(n)
- 이분탐색
    - n건에 대해 log n번만에 목적 데이터를 찾는 알고리즘 O(log n)
- 최대 탐색횟수가 계산량이라고 할 수 있고, 일반적으로 계산량이 적을수록 속도가 빠르다
#### 제7장, 두 가지 목적
- 대규모 데이터를 앞둔 알고리즘 선택의 중요성을 느끼는 것
- 실제로 알고리즘을 제품에 전개하기까지 어떤 과정이 있는지 알아보는 것
### 알고리즘이란?
- 어떤 값 또는 집합을 입력으로 하고 어떤 값 또는 값의 집합을 출력으로 하는 명확하게 정의된 계산절차
#### 좁은 의미의 알고리즘, 넓은 의미의 알고리즘
- 처리의 흐름 (Domain Logic) : 넓은 의미
- 명확하게 정의된 계산문제에 대해 정의된 계산절차를 수행하는 것 : 좁은 의미
### 알고리즘을 배우는 의의
- 컴퓨터의 자원은 유한, 엔지니어의 공통언어
- 알고리즘은 새로운 문제에도 대처할 수 있게끔 함
### 알고리즘의 평가
- Order 표기
- 입력크기가 n일 때 소요되는 계산량
- n의 크기에 관계없이 일정한 시간에 끝나는 경우에는 O(1)
    - hash
- 특정 상황을 다루는 것이 아니라 평균 또는 최대를 평가
#### 각종 알고리즘의 Order 표기
- O(1) < O(log n) < O(n) < O(nlog n) < O(n¹) < O(n²) ... 
- 대상이 되는 계산에 따라 빠르다 할만한 정도가 다름
- 일반적인 정렬 알고리즘은 아무리 잘해도 O(nlog n)보다 빠를 수 없음
### 티슈를 몇 번 접을 수 있을까
- O(log n)과 O(n)의 차이 
    - 종이접기 예시
#### 알고리즘에 있어서 지수적, 대수적 감각
- 계산량이 지수적으로 증가하는 알고리즘은 데이터 량이 적어도 계산량이 매우 커져버림
- 지수의 역인 대수적으로만 증가하는 O(log n)인 알고리즘은 데이터 량이 꽤 커져도 적은 계산량으로 문제 해결 가능
### 알고리즘과 데이터 구조
- 뗄레야 뗄 수 없는 관계
- 데이터 구조는 배열, 트리구조와 같이 대상이 되는 데이터를 저장 또는 표현하기 위한 구조
- 알고리즘에서 자주 사용하는 조작에 맞춰 데이터 구조를 선택할 필요가 있음
### 계산량과 상수항
- 역시 측정이 중요
- Order 표기에서는 상수향을 무시
- 알고리즘을 구현하는 입력 크기에는 의존하지만 실행하지 않으면 안되는 처리
- 상수항은 알고리즘 계산량에 거의 영향을 주지 않지만, 복잡해지면 무시 불가능
#### 구현 시 유의하고픈 최적화 이야기
- 상수항을 줄이기 위해 처음부터 최적화를 수행하는 것은 잘못된 방침
- 측정이 중요, 벤치마크를 하거나 프로파일링 할 것
### 알고리즘의 실제 활용
- 단순한게 더 낫기도?
- 고도의 알고리즘이 반드시 최고의 해법은 아님
#### 하테나 북마크 Firefox 확장기능인 검색기능에서의 시행착오
- 증분검색이라면 검색도 상단한 빈도로 발생하고 클라이언트에서 계산되니까 계산량을 적게 가져가자
- Suffix Array 사용 결정
    - 미리 전처리를 거친 데이터 구조를 만들 필요가 있음
    - 전처리에 상당한 시간이 필요함
- like 부분일치 검색으로 변경 (선형 탐색)
    - 속도 저하를 우려했으나 그닥 문제 없었음
### 써드파티 소스를 잘 활용하자
- 정평이 난 알고리즘은 제 3자가 이용하기 쉽도록 이미 구현된 소스가 공개되어 있는 경우가 많음
- 원하는 사양으로 되어있지 않다던가, 오버스펙인 경우도 있긴함
### 데이터 압축과 속도
- 전체적인 처리량을 높이기 위한 사고방식
- 압축 해제 처리는 무겁다거나 느리다고 생각할 수 있지만, 처리량 관점에서는 빠른 경우가 많음
- HTTP의 deflate 압축통신이 좋은 예시
## 7-20. 하테나 다이어리의 키워드 링크
### 키워드 링크란?
- 글을 작성하면 일부 키워드에 링크가 자동으로 걸림
- 입력된 전문에 대해 키워드 사전과 매칭해서 필요한 부분을 링크로 치환
### 최초 구현 방법
- 사전 내에 포함된 모든 단어를 OR 조건으로 잇는 정규표현식 사용
### 문제발생!
- 키워드는 사용자가 등록하는 것
- 키워드가 많아질 수록 정규표현 처리에 시간이 걸림
    - 정규표현을 컴파일 하는 처리
    - 정규표현에서 패턴매칭하는 처리
### 패턴매칭에 의한 키워드 링크의 문제점
- 키워드 어휘수, 하테나 다이어리 엑세스 수 증가로 키워드 링크 처리 횟수도 늘어났고, 시스템이 비명을 지름
- 정규표현은 패턴매칭 구현에 오토마톤을 사용
- 대부분의 실용적인 언어에서 NFA (Nondeterministic Finite Automata)  이용
    - 앞에서부터 입력값을 살펴가면서 매칭에 실패하면 다음 단어를 시도, 반복
### 정규표현 -> Trie
- 패턴매칭에 수반되는 계산량 문제를 해결하기 위해 정규표현에서 Trie(트라이)를 사용한 매칭 구현으로 변경
#### Trie 입문
- Trie는 트리구조의 일종인 데이터 구조
- 탐색대상 데이터의 공통 접두사를 모아서 트리구조를 이루는게 그 특징
#### Trie 구조와 패턴매칭
- Trie 구조를 사전과 비교하면서 패턴매칭을 하면 정규표현인 경우보다 계산량을 줄일 수 있음
### AC법
- Trie에 의한 매칭을 더욱 빠르게
- Aho-Corasick법
- 사전 내에서 패턴매칭을 수행하는 오토마톤을 구축하고 입력 텍스트에 대해 선형 계산시간을 실현
- 계산량이 사전 크기에 의존하지 않는 빠른 방법
- Trie의 패턴매칭으로 매칭이 진행되다가 도중에 실패했을 경우, 되돌아오는 길의 엣지를 다시 Trie에 추가한 데이터 구조
### Regexp::List로의 치환
- 나중에는 Regexp::List CPAN 라이브러리로 치환함
- Perl 최적화 및 정규표현식 사용 가능
### 키워드 링크 구현, 변이 및 고착
- 정규표현 -> AC법 -> Regexp::List
- 당초 심플한 구현이었던 것이 주효했었음
    - 공수가 적었음
    - 구현의 유연성이 풍부했음
- 데이터가 커짐으로써 문제가 혼재화 되는 경우
    - 캐시 등 표면적인 변경으로 회피했으나,
    - 알고리즘이 갖는 근본적인 문제점을 해결해야만 했음
- 처음부터 최적의 구현을 사용하는 것이 반드시 옳다고 할 수 없음
## 7-21. 하테나 북마크의 기사 분류
### 기사 분류란?
- 새로 도착한 기사를 해당 기사의 내용을 기반으로 자동으로 카테고리를 분류
#### 베이지안 필터에 의한 카테고리 판정
- 베이지안 필터
    - 스팸필터 등에 응용
    - 텍스트 문서 등을 입력으로 받아들이고 거기에 나이브 베이즈라고 하는 알고리즘을 적용
    - 확률적으로 해당 문서가 어느 카테고리에 속하는지를 판정하는 프로그램
    - 사전에 수동으로 정답이 되는 데이터(정해 데이터를) 주고 프로그램을 학습시켜 최종적으로는 개입하지 않는 프로그램
    - 기계학습 분야
    - 패턴에 따라 분류 - 패턴인식
### 기계학습과 대규모 데이터
- 많은 기계학습 태스크에는 정해 데이터가 필요함
- 데이터가 많을수록 정밀도가 향상되는 경우가 드물지 않음
#### 하테나 북마크의 관련 엔트리
- 관련 엔트리 : 특정 기사와 매우 비슷한 다른 관련정보를 사용자에게 제시
- 4,000만 건 이상의 '태그'라는 분류용 텍스트를 입력으로 기사추천 알고리즘을 사용해서 실현
### 대규모 데이터와 웹 서비스
- 구글 검색중 잘못된 검색 쿼리에 대해서 정확한 쿼리를 추천해주는 기능
- 과거 사용자 검색을 정해 데이터로 해서 학습을 하고, 정해 데이터를 제시하는 듯 함
- 전례없는 규모의 대량 데이터를 쏟아부으면 블랙박스에서도 정답이 나옴
### 베이지안 필터의 원리
- 베이지안 필터의 핵심은 나이브 베이즈 알고리즘
#### 나이브 베이즈에 근거한 카테고리 추정
- 문서 D가 주어졌을 경우, 카테고리 C일 조건부 확률
- P(C|D)
- 여러 카테고리 중 확률이 가장 높은 값을 나타낸 C가 선택
- P(C|D) = P(D|C) P(C) / P(D)
- P(D)는 문서가 발생할 확률
    - 모든 카테고리에 대해 동일하므로 무시 가능
- P(C)는 카테고리가 출연할 확률
    - 학습 데이터 중 여러 데이터가 어떤 카테고리로 분류되었는지 횟수를 저장하면 나중에 계산 가능
- P(D|C)
    - 문서 D가 임의의 단어 W가 연속으로 출연하는 것으로 간주
    - P(W1|C) P(W2|C) P(W3|C) ... P(Wn|C) 와 같이 근사해볼 수 있음
    - 단어별로 어떤 카테고리로 분류되었는지 그 횟수를 보존하면 P(D|C)의 근삿값을 구할 수 있음
#### 베이즈 정리
- P(B|A) = P(A|B) P(B) / P(A)
- A가 일어난 다음에 B가 일어날 확률을 구하기 어려울 때 도움이 됨
#### 손쉬운 카테고리 추정 실현
- 나이브 베이즈에서는 정해 데이터가 주어지면 사용된 횟수나 단어 출현 횟수 같은 간단한 수치만 저장해두면 나중에 확률만 계산해서 카테고리 추정 가능
- 일부 정해 데이터로 약간의 확률계산을 하기만 하면 되므로 빠르게 처리 가능
### 알고리즘이 실용화되기까지
- 베이지안 필터는 구조상 의외로 심플
- 실제 사례
    - C++ 분류엔진을 서버화
    - 서버와 통신해서 결과를 얻는 Perl 클라이언트 작성 후 웹 애플리케이션에서 호출
    - C++ 엔진에 덤프/로드 기능추가
    - 1,000 건의 학습 데이터 수작업 준비
    - 통계 구조 작성 후, 그래프화하면서 정밀도 튜닝
    - 다중화를 고려하여 스탠바이 시스템 구축
    - 웹 애플리케이션에 사용자 인터페이스 마련
#### 실무 면에서 고려해야 할 점은 꽤 많다
- 현장에서는 이 작업이 올바르게 수행되도록 관리
- 사전에 공수에도 반영해두는 것도 중요
### 수비 자세, 공격 자세
- 기사 분류 구현으로부터의 고찰
- 기계학습, 패턴인식, 데이터마이닝 기법
    - 대량의 데이터에서 의미있는 데이터를 일괄 추출
    - 대규모 데이터의 '특징'을 컴팩트하게 갖고 있다가 이용 가능
- 수비적인 자세
    - 대규모 데이터 정렬, 검색, 압축하는일
    - 발생하는 문제를 얼마나 잘 맞아들이는가
- 공격적인 자세
    - 기계학습이나 패턴인식
    - 대규모 데이터를 응용하고 결과에 따라 애플리케이션에 부가가치를 추가
#### 기존 방법 익혀두기
- 대규모 데이터에 대해 알고리즘 측면에서의 접근법을 배우려고 할 때 기존 방법은 어느 정도 자신의 지식으로 익혀두는 것이 중요
#### 스펠링 오류 수정기능 만드는법
- 하테나의 스펠링 오류 수정기능
1. 27만 단어의 정해 사전 - 정해 데이터로는 하테나 키워드를 사전으로 사용
    - 스펠링 오류 수정 프로그램은 정해가 무엇인지 알고 있을 필요가 있는 엔진
2. 검색 쿼리와 사전 내의 어구 사이의 편집거리를 구해서 "오류 정도"를 정량화
    - 편집거리 : 한 단어에서 다른 단어로 바꿔 쓸 때, 그 횟수가 몇 회가 되는지에 따라 단어 간 거리를 정량화 한 것
    - 동적계획법 알고리즘으로 간단하고 빠르게 구현
3. 일정한 오류 정도를 기준으로 사전 내에 있는 단어군을 정해 후보로 얻어낸다
    - 입력 쿼리와 사전 내의 단어 간 편집거리를 비겨ㅛ해서 값이 작은 것의 목록을 얻어냄
    - 사전의 n-gram 인덱스를 만들어두고 입력어구와의 bi-gram의 중복 정도가 높은 단어만 추출할 수 있는 데이터 구조
    - 위 데이터 구조로 사전에 비교할 대상의 범위를 줄여두고, 그 줄어든 대상에 대해 편집거리를 하나씩 구해감
4. 정해 후보를 하테나 북마크 기사에서의 단어 이용빈도를 기준으로 정해에 가까운 순으로 정렬
    - 얻어진 정해 후보는 편집거리가 1,2등으로 작은 값이므로 복수의 후보가 얻어지는 경우가 많음
    - 대상으로 하는 검색 공간에서 가장 빈번하게 나타나는 단어가 정해
5. 가장 이용빈도가 높은 단어를 정해로 간주하고 이용자에게 제시
    - 정해에 가장 가까운 단어를 사용자에게 보정 후보로 제시
