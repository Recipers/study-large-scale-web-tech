# 07장 - 알고리즘 실용화
## 알고리즘과 데이터 구조 선택의 중요성
- 문제해결에 적합한 알고리즘과 데이터 구조를 사용하면 하루 종일 걸리던 계산이 수 초만에 해결되기도 함
- 강의 19 -> 알고리즘 평가 (Order 표기 등)
- 강의 20, 21 -> `키워드 링크(하테나 다이어리)`, `기사 분류(하테나 북마크)`를 바탕으로 알고리즘 사용 방식 소개

## 강의19: 알고리즘과 평가
### 데이터 규모와 계산량 차이
- 1000건의 데이터 중 특정 값을 찾을 때 선형 탐색이라면 최대 1000번, 이진 탐색이라면 최대 10번
    - 이때 탐색횟수는 계산횟수의 기준이 되는 `계산량`이라고 표현 -> 계산량이 작을수록 속도가 빠름
    - 선형 탐색일 때 데이터가 백만 건이면 최대 100만, 이진 탐색이라면 20번이면 끝남
    - 선형 탐색의 비해 이진 탐색이 데이터량 증가에 훨씬 강함

#### 7장의 두 가지 목적
1. 대규모 데이터를 앞둔 알고리즘 선택의 중요성을 알기
2. 실제로 알고리즘을 제품에 전개하기까지 어떤 과정이 있을지 살펴보는 것

### 알고리즘이란?
- 넓은 의미와 좁은 의미가 있음
    - 프로그램의 큰 틀에서 알고리즘을 논할 때의 알고리즘은 도메인 로직의 흐름일 가능성이 높음
    - 반대로 작은 틀에서 알고리즘이면 명확하게 정의된 계산문제에 대해 정의된 계산절차를 수행하는 것

### 알고리즘을 배우는 의의 - 컴퓨터의 자원은 유한, 엔지니어의 공통언어
- CPU, 메모리 등 컴퓨터의 자원은 유한하기 때문에 알고리즘 중요
    - 지금 해결해야 할 문제를 유한한 자원으로 어떻게 해결해야 할 지
- 디자인패턴과 마찬가지로 엔지니어의 공통 언어
    - ex) 그 부분은 해시로 해두면 좋다 -> 해시를 알아야 대화가 된다.

### 알고리즘의 평가 - Order 표기
- Big O 표기법과 동일

### 티슈를 몇 번 접을 수 있을까? - O(log n)과 O(n)의 차이
- 티슈를 7번 접는 것도 힘듦 (0.11mm 25번 접으면 후지산 높이)
- 이와 같이 `log n`과 `n`의 차이는 엄청남

### 알고리즘과 데이터 구조
- `데이터 구조` : `배열`, `트리`와 같이 대상이 되는 데이터를 저장 또는 표현하기 위한 구조
- 알고리즘에서 자주 사용하는 조작에 맞춰 데이터 구조를 선택할 필요가 있음
    - ex) 적절한 트리 구조로 데이터를 저장하면 대게 탐석처리를 줄일 수 있어 계샨량을 줄일 수 있음
    - RDB에서는 B+트리가 자주 사용

### 계산량과 상수항 - 측정 중요
- 다른 정렬 알고리즘도 log n인데 일반적으로 퀵 정렬이 가장 빠름, 이는 CPU 캐시를 사용하기 쉽게 되어 있음
    - 즉, Order 표기는 알고리즘을 비교할 때 편리하지만 구현을 포함해 생각하면 그게 전부는 아님

### 알고리즘의 실제 활용
- 고도의 알고리즘이 반드시 최고의 해법이 아님, 고전적인 알고리즘이 더 좋은 경우도 있음
- 같은 맥락으로 잘 알려진 알고리즘보다 단순한 알고리즘이 더 좋을 수도 있음

#### 하테나 북마크 Firefox 확장기능인 검색기능에서의 시행착오
- 과거에 사용자가 북마크한 데이터를 검색할 수 있게 함
    - `Suffix Array를` 사용하기로 결정하고 구현
        - `Suffix Array` : 텍스트 데이터 등을 고속으로 검색하기 위한 데이터 구조
        - 탐색 자체는 빠르지만 전처리를 거친 데이터 구조를 만들어야 하므로 전처리에 시간이 많이 걸림
    - 이렇게 만들었던 Suffix Array가 기대한 만큼 동작하지 않음
        - Firefox 확장 기능의 내부적으로 있던 SQLite에 SQL로 like 연산으로 변경
        - 느릴줄 알았는데 전혀 문제가 없었음

### 서드파티 소스를 잘 활용하자 - CPAN 등
- 오픈소스로 된 각종 알고리즘 라이브러리 소스가 많이 공개되어 있음 잘 찾아보자.
    - 그렇다고 알고리즘 내용이 블랙박스인 채로 사용하지는 말자

- - -
## 강의20: 하테나 다이어리의 키워드 링크
### 키워드 링크란?
- 블로그에 글을 작성하면 일부 키워드에 링크가 자동으로 걸리는 방식
    - Wiki 워드에 링크하는 기능이랑 비슷
- 링크 대상 키워드는 하테나 사용자가 등록한 키워드 (09년 08월 기준 27만 단어)
    - 일일 대략 100개 정도 새로운 키워드 등록
- 따라서 `입력된 전문에 대해` 27만 단어를 포함하는 `키워드 사전과 매칭 후` **필요한 부분을 링크로 치환**하는 것이 `키워드 링크`
    - 특정 키워드를 HTML anchor 태그로 치환하는 것 뿐 -> 문장 내 키워드 위치를 텍스트 치환하는 문제
    - ex) 하테나 다이어리는 ~~ ->`<a href="....">하테나 다이어리</a>`는 이런 식으로 `<a>` 태그를 추가

### 최초 구현방법
- 특별한 연구 없이 사전 내 포함된 모든 단어를 OR 조건으로 잇는 정규표현식 만들어 사용
    - (foo|bar|baz| ...) 이런 식
    - $text 라는 변수에 텍스트가 들어 있다면 치환 옵션과, 치환 문자열을 식으로 평가하는 eval 옵션으로 개발

### 문제 발생 - 키워드 사전의 대규모화
- 키워드수가 많아짐에 따라 문제 발생 특히 `(1) 정규표현 컴파일 처리`, `(2) 정규표현 패턴매칭 처리`가 시간이 많이 소요
- 1은 미리 정규표현을 만들어나 메모리나 디스크 상에 저장 즉, 캐싱해서 회피가 가능했다.
- 2는 처음에 키워드 링크가 완료된 본문 텍스트를 캐싱하는 식으로 회피했다가 `새로 추가되는 키워드를 키워드 링크에 반영시키기 위해서 일정 시간에 캐시를 다시 구축`하거나, `그다지 액세스가 없는 블로그에서는 캐시하는 것이 의미가 없으므로` 근본적인 해결은 어려웠음

### 패턴매칭에 의한 키워드 링크의 문제점
- 정규표현은 패턴매칭 구현에 automaton을 사용하고, Perl의 정규표현 구간에는 **NFA**가 사용
    - Perl뿐 아니라 많은 언어 대부분이 NFA 엔진의 정규표현을 사용
    - **NFA(Nondeterministic Finite Automate)** 는 앞에서부터 입력값을 살펴가며 매칭에 실패하면 다음 단어를 시도
        - ex) `(foo|bar|baz)`를 예로 보면 `foo 매칭 -> bar 매칭 -> baz 매칭` 이렇게 키워드 개수에 비례하는 계산량 소요
- 서비스 초기에는 키워드 개수가 적어 계산량 또한 적어서 가능했음

### 정규표현 -> Trie - 매칭 구현 변경
- 정규 표현에서 Trie로 변경

#### Trie 입문
- 트리구조의 일종인 데이터 구조
- 탐색대상 데이터의 공통 접두사를 모아서 트리구조를 이루는 게 특징

#### Trie 구조와 패턴매칭
- 정규표현 방식에 비해 공통 접두사는 한 번의 탐색으로 찾을 수 있음 - 단어의 길이만큼만 계산하면 끝

#### AC(Aho-Corasick)법 - Trie에 의한 매칭을 더욱 빠르게
- Trie 구조에 의한 패턴매칭을 더욱 빠르게 하는 **AC법** 이용
- AC법은 **사전 내에서 패턴매칭을 수행하는 오토마톤을 구축**하고 입력 텍스트에 대해 선형 계산시간을 실현
- Trie에서 패턴매칭으로 찾다가 도중에 `실패했을 경우 되돌아오는 길의 엣지를 다시 Trie에 추가한 데이터 구조` 사용
- AC법을 이용해 키워드 링크를 구현하는 과제는 **8장 참고**

### Regexp::List로의 치환
- AC법을 도입해서 키워드 링크의 계산량 문제는 해결
    - AC법을 직접 구현한 형태로 사용하다 Regexp::List라는 CPAN 라이브러리로 변경

### 키워드 링크 구현, 변이 및 고찰
- `정규표현` -> `AC법` -> `Regexp::List`로 변화를 통해 얻은 점
    - 단순한 구현은 구현에 걸리는 공수가 적어 유연성이 풍부했음 -> 이로인해 사용자 요청에 맞게 변경이 쉬움
    - 데이터가 커짐으로 문제가 혼재화되는 경우 발생 -> 이를 위해 본질적인 해결책 필요
        - 캐시 등 표면적인 변경으로 어느 정도는 문제를 해결할 수 있지만, 최종적으로 알고리즘이 갖는 근본적인 해결책 필요
- 처음부터 최적의 구현을 사용하는 것이 좋지 않음
    - 데이터가 작은 동안에는 오히려 결과도 좋음. -> 대규모 데이터가 될 시기에는 본질적인 문제의 해결방법도 생각해봐야 함

- - -
## 강의21: 하테나 북마크의 기사 분류
### 기사 분류란?
- 기사의 내용을 기반으로 자동으로 카테고리를 분류하는 기능
    - `과학, 학문`, `컴퓨터, IT`, ... 이런 식으로 전부 합쳐 8개의 카테고리 존재
    - 새로운 기사가 작성되면 해당 기사를 HTTP로 얻어 본문 텍스트의 내용을 분류해 **카테고리 판정**

#### 베이지안 필터에 의한 카테고리 판정
- **카테고리 판정**에는 `베이지안 필터` 사용
    - `베이지안 필터` : 텍스트 문서 등을 입력으로 받아 나이브 베이즈(Naive Bayes)라는 알고리즘을 적용해 **확률적으로 해당 문서가 어느 카테고리에 속하는지 판정**하는 프로그램
    - 기계학습을 기반으로 동작 (수동으로 정답이 되는 데이터를 주고 이를 학습시켜 사용)
        - 패턴인식 분야의 일종 - 패턴인식 분야의 알고리즘을 잘 응용하면 카테고리 자동분류 같은 기능 개발 가능

### 기계학습과 대규모 데이터
- 많은 기계학습은 베이지안 필터와 같이 정해 데이터가 필요 - 베이지안 필터의 경우 많은 정해 데이터가 필요하지는 않음

#### 하테나 북마크의 관련 엔트리
- 특정 기사와 비슷한 다른 관련정보를 제공하는 기능
- 4천만 건 이상의 사용자 손으로 입력된 `태그`라는 분류용 텍스트를 입력으로 `기사추천 알고리즘`을 사용해 구현
    - 기사추천 알고리즘은 Preferred Infrastructure 엔진 사용
    - 수천만 건에 태그 데이터를 이용해 수 건의 기사를 꺼내오는 방식, 사람이 하기에는 어려움

### 대규모 데이터와 웹 서비스 - The google Way of Science
- `대량의 데이터와 응용수학이 다른 모든 도구를 대신한다.` 라는 취지를 가진 Kevin Keylly의 컬럼 소개

### 베이지안 필터의 원리
- **나이브 베이즈 알고리즘**이 핵심
    - 베이즈의 정리라는 공식을 기반으로 하는 알고리즘

#### 나이브 베이즈에 근거한 카테고리 추정
- 특정 문서 D가 주어졌을 때 해당 문서가 확률적으로 어떤 카테고리 C에 속하는게 그럴 듯 한지 구하는 문제
    - 문서 D가 주어졌을 때 카테고리 C인 조건부 확률을 구하는 문제 `P(C|D)`
    - 여러 카테고리 중 확률이 가장 높은 값을 나타낸 C가 최종 선택
    - 조건부 확률을 구할 때 `베이즈의 정리`를 이용 - `P(C|D) = P(D|C) P(C) / P(D)`
        - 우변의 각 확률 `P(D|C) P(C) / P(D)`를 구하는 문제로 생각할 수 있음

> Memo - 베이즈 정리
>
> `P(B|A) = P(A|B) P(B) / P(A)`
> - 베이즈 정리는 위 확률공식과 같은 구성으로 나타냄
    > 	- `P(B|A)`, 즉 사상 A가 일어난 다음에 사상 B가 일어날 확률을 직접 구하기 어려울 때 도움이 됨
           > 	- 베이즈 정리로 변형하면 `P(B|A)`를 구하는 문제에서 `P(A|B)`, `P(B)`, `P(A)`를 알면 되는 문제로 변형
> 	- `P(A)`는 다른 값과 비교했을 때 무시할 수 있는 경우가 많아 결과적으로 `P(A|B)`, `P(B)`만 알면 됨

#### 손쉬운 카테고리 추정 실현
- 나이브 베이즈에서는 정해 데이터가 주어지면 해당 정해 데이터가 사용된 횟수나 단어 출현횟수와 같이 수치만 저장해두고, 나중에 확률만 계산하면 카테고리를 추정할 수 있게 됨 (그 밖의 데이터는 전부 파기해도 됨)
- 하테나의 경우 2천만 건 이상의 데이터가 저장되어 있고, 나이브 베이즈는 그 중 일부 정해 데이터, 그 중에서도 일부 데이터만 저장해두면 되므로 대량 데이터라 해도 엔진 자체는 컴팩트함
- 또한 카테고리를 추정할 때 수행하는 계산도 일부 정해 데이터로부터 약간의 확률 계산만 하면 되므로 빠르게 처리 가능
- `이렇게 대규모 기사군의 내용을 보고 각각의 카테고리를 자동으로 판정하라`는 문제를 해결

### 알고리즘이 실용화되기까지 - 하테나 북마트의 실제 사례
- 베이지안 필터는 구조상 심플, 실제 구현해봐도 스크립트 언어로 대략 100~200줄 정도, 알고리즘 구현 자체는 간단한 편
- 그렇다면 프로덕션 환경에 올리려먼 어떤 작업이 남았을까?
    - 분류 엔진은 C++로 개발 됨 -> 해당 엔진 서버화
    - 이 서버와 통신해서 결과를 얻는 Perl 클라이언트를 작성하고 웹 애플리케이션에서 호출
    - 학습 데이터를 정기적으로 백업할 수 있도록 C++ 엔진에 데이터 덤프/로드 기능 추가
    - 학습 데이터 천 건을 수작업으로 준비 -> 사람이 애써야 됨
    - 바람직한 정밀도가 나오는지 추적하기 위한 통계 구조 작성 -> 그래프화하면서 정밀도 튜닝
    - 다중화를 고려해 스탠바이 시스템 구축 -> failover는 공수가 많이 소요되므로 백업에서 로드할 수 있는 정도로 타협
    - 웹 애플리케이션에 사용자 인터페이스 마련
- 서버화나 Perl과의 API 교환에는 Apache Thrift(RPC 프레임워즈 중 하나) 사용

### 수비 자세, 공격 자세 - 기사 분류 구현으로부터의 고찰
- 동일한 알고리즘이라도 대규모 데이터를 빠르게 정렬, 검색, 압축하는 일은 발생하는 문제를 얼마나 잘 버티는 가라는 `수비 자세`
- 기계학습이나 패턴인식 등은 대규모 데이터를 응용하고 결과에 따라 애플리케이션에 부가가치를 추가한다는 의미로 `공격 자세`

#### 기존 방법 익혀두기
- 공/수 모두 기존 방법을 알고 있어야 응용할 수 있음
    - 키워드 링크에 Trie를 응용한다는 발생이 Trie가 가지는 특성을 모른다면 생각할 수 없음

### Column - 스펠링 오류 수정기능 만드는 법 (ex. 하테나 북마크 검색 기능)
- Google에서 `이것을 찾으셨나요?` 기능
- 검색쿼리를 보정하는 기능은 검색엔진의 로그를 정해 데이터로 한 학습엔진에 의해 구현
    - 사용자들이 검색했던 방식을 기반으로 제안
- 구글처럼 **대량의 로그가 없을 때** 구현하는 방법 - 하테나 북마크의 방식
    - 특정 사전 데이터를 정해로 해서 잘못된 해답에 보정을 가하는 방식
    1. 정해 데이터로 27만 단어가 있는 하테나 키워드를 사전으로 사용
    2. 사용자가 입력한 검색쿼리와 사전 내의 어구 사이의 편집거리를 구해 `오류 정도`를 정량화
    3. 일정 오류 정도를 기준으로 사전 내에 있는 단어군을 정해 후보 찾기
    4. (3)의 정해 후보를 하테나 북마크 기사에서의 단어 이용빈도를 기준으로 정해에 가까운 순으로 정렬
    5. 가장 이용빈도가 높은 단어를 정해로 간주하고 이용자에게 제안
    - 각 단계에 대한 자세한 내용은 책 `188 ~ 190p` 참고
