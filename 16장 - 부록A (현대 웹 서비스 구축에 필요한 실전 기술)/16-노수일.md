# 현대 웹 서비스 구축에 필요한 실전기술
### 대규모 서비스에 대응하기 위해서
## 성장하는 서비스, 계속 증가하는 데이터
### 작업큐, 스토리지, 캐시 시스템, 계산 클러스터
- 웹서비스 구축을 위한 실전적인 기술
- 작업큐 시스템
- 스토리지 선택 (RDBMS, Key-value 스토어)
- 캐시 시스템
- 계산 클러스터
## 1. 작업큐 시스템
### 웹 서비스와 요청
- 웹 서비스에서는 기본적으로 요청이 동기적으로 실행
- 요청에 기인하는 모든 처리가 끝난 다음 응답
- 웹 서비스가 성장하면서 데이터 추가 및 갱신이 점점 무거워짐
- 양호했던 성능이 시간이 지남에 따라 악화됨
- 작업큐 시스템을 사용함으로써 나중으로 미뤄도 되는 처리를 비동기로 실행할 수 있음
### 작업큐 시스템 입문
- 가장 간단한 비동기화 처리
    - 비동기화 처리를 독립된 스크립트로서 해당 스크립트를 애플리케이션 내부에서 호출하는 방법
    - 스크립트 시작과 초기화의 오버헤드가 커서 성능이 좋진 않음
    - 일시적으로 대량의 비동기 처리를 실행시키려하면 그 수만큼의 프로세스를 실행시키기 때문에 성능상 단점이 됨
- 어느정도 양이 있는 비동기 처리를 안정적으로 수행하려면 작업큐와 워커를 세트로 한 작업큐 시스템을 사용하는 것이 일반적
- 작업큐에 실행하고자 하는 처리를 등록하고, 워커가 큐에서 작업을 추출해서 실제로 처리
- 작업큐를 통해서 일시적으로 대량의 처리가 등록되었을 때 부하의 변동을 흡수할 수 있음
- 워커는 항상 실행해둠으로써 작업을 처리할 때 초기화 오버헤드를 거의 없앨 수 있음
- 클라이언트
    - 작업을 투입, 다음 처리 계속 진행
- 작업큐
    - 작업을 쌓음
- 워커
    - 작업큐를 참조하고 미실행된 작업을 추출해서 작업을 실행
### 하테나에서의 작업큐 시스템
- 하테나의 작업큐 시스템은 TheSchwartz와 Gearmean 사용
#### TheSchwartz
- RDBMS 사용하는 작업큐 시스템
- 높은 신뢰성
- 속도에서는 다소 희생이 따름
#### Gearman
- TheSchwartz보다 가벼운 작업큐 시스템
- 독자적인 데몬을 사용해서 작업의 정보를 메모리에 저장
- 성능은 확보했지만 신뢰성에 희생이 따름
- 클라이언트에서 작업을 투입할 때 세 가지 패턴을 취함
    - 동기적으로 순번대로 처리
    - 동기적으로 병렬로 처리
    - 비동기적으로 백그라운드로 처리
#### WorkerManager에 의한 워커 관리
- 워커 프로세스를 세세하게 제어하기 위한 하테나 자체 개발 툴
- TheSchwartz와 Gearman을 래핑해서 최소한의 변경으로 양쪽으로 대응
- 설정파을로 워커 클래스 정의, 설정파일만 수정해서 워커로서 사용할 클래스 변경
- 워커 프로세스의 라이프사이클 관리, 프로세스 관리, 데몬화 수행
- 워커 포르세스의 프로세스 개수 관리, 프로세스 개수를 관리하고 병행처리가 가능한 작업수를 제어
- 로그 출력, 작업을 처리한 타임스탬프 기록
- Apache의 prefork 모델을 참고하여 부모 프로세스에서 지정된 수의 자식 프로세스를 생성
- 자식 프로세스별로 처리할 작업수를 지정시킬 수 있음
### 로그 분석
- 처리시간과 지연시간을 측정함으로써 투입된 작업 종류와 양에 대해 워커의 처리능력이 충분한지 여부를 확인할 수 있음
## 2. 스토리지 선택
- RDBMS와 Key-value 스토어
### 증가하는 데이터를 어떻게 저장할까
- 수십GB, 수백GB, TB를 넘는 데이터를 다루는 스토리지는 약간의 구성 변경이나 엑세스 패턴 변화로 응답속도가 저하되는 경우가 있음
- 데이터량이나 스키마, 엑세스 패턴에 맞는 스토리지를 선택하는 것은 대단히 중요
#### 웹 애플리케이션과 스토리지
- 스토리지 : 애플리케이션 데이터를 영속적으로 혹은 일시적으로 저장하기 위한 기능
- 사진, 블로그 본문 등의 원본 데이터
    - 가장 중요
    - 근본적인 신뢰성과 관계, 최상급 신뢰성을 확보
- 원본데이터를 가공함으로써 생성된 랭킹, 인덱스 등의 가공 데이터
- 캐시와 같이 사라져도 성능상 문제 이외에 문제가 없는 데이터
    - 성능을 높이거나 비용을 줄일 필요가 있음
#### 적절한 스토리지 선택의 어려움
- 스토리지 설계와 구현은 다양한 종류가 제안되어 왔음
- 스토리지를 잘못 선택한 채로 서비스를 시작하게 되면 변경은 뜻대로 이룰 수 없음
- 테라바이트 규모의 데이터를 서비스에 영향을 주지 않고 다른 스토리지로 옮기는 것은 세심한 주의, 시간이 많이 걸림
### 스토리지 선택의 전제가 되는 조건
- 애플리케이션에서의 엑세스 패턴을 이해하는 것이 중요
- 여섯 가지 지표
    - 평균크기
    - 최대크기
    - 신규추가빈도
    - 갱신빈도
    - 삭제빈도
    - 참조빈도
- 신뢰성, 허용할 수 있는 장애 레벨, 사용할 수 있는 하드웨어나 예산
### 스토리지의 종류
- RDBMS : MySQL, PostgreSQL 등
- 분산 key-value 스토어 : memcached, TokyoTyrant 등
- 분산 파일시스템 : MogileFS, Lustre
- 그 밖의 스토리지 : NFS 계열 분산 파일시스템, DRBD, HDFS
### RDBMS
- 표 형식으로 데이터를 저장
- SQL 언어로 데이터 조작을 수행
- 다양한 데이터를 저장, 강력한 질의로 가장 범용성이 높음
#### MySQL
- SQL을 해석해서 실행하는 기능 블록과 실제 데이터를 보관하는 기능 블로깅 분리되어 있음
- 후자는 스토리지 엔진, 다양한 종류가 개발, 구현되고 있음
- 스토리지 엔진 : MyISAM, InnoDB, Maria
#### MyISAM
- MySQL 5.1의 표준 스토리지 엔진
- 1개의 테이블이 실제 파일 시스템 상에 3개의 파일(정의, 인덱스, 데이터)로 표현
- update, delete를 한 적 없는 테이블에 insert 조작을 빠르게 할 수 있음
- 시작, 정지가 빠르며 테이블 이동이나 이름변경을 파일시스템 조작으로 직접 할 수 있는 등 운용 용이
- DB 프로세스가 비정상 종료하면 테이블이 파손될 가능성이 높음
- 트랜잭션 기능이 없음
- update, delete, insert가 테이블 락으로 되어 있어서 성능적으로 불리
#### InnoDB
- MyISAM과는 대조적인 스토리지 엔진
- 스토리지 엔진 전체에 사전에 정의한 소수의 파일에 데이터를 저장
- 트랜잭션 지원
- 비정상 종료 시 복구 기능이 있음
- 데이터 기능이 로우락으로 되어 있음
- 데이터량에 따라서는 시작, 정지가 수 분 정도 걸린다거나 테이블 조작을 모두 DB를 경유해서 수행해야 함
#### Maria
- MyISAM의 후속
- MyISAM에서 트랜잭션 및 비정상 종료 시 복구 기능 추가
#### MyISAM vs InnoDB
- 애플리케이션 기능이나 특성을 고려해서 적절한 스토리지 엔진 선택
- 1대의 서버에 두 가지를 혼용하지는 않음
- 각기 다른 동작, 메모리 사용법을 취하므로 혼용하는 환경에서는 효율적인 CPU, 메모리 사용이 어려워짐
### 분산 Key-value 스토어
- key와 value 쌍을 저장하기 위한 심플한 스토리지
- 네트워크를 지원함으로써 다수 서버로 확장시키는 기능을 지님
- RDBMS에 비해 기능적으로 부족함
- 성능은 10~100배 이상이라는 것이 특징
- memcached가 가장 유명
    - 메모리 상에서 동작
    - 매우 빠름
    - 재시작 시 데이터가 모두 사라짐
- TokyoTyrant가 주목됨
    - 디스크 상에 DB파일을 가짐
    - 재시작 후에도 데이터가 보존됨
#### memcached
- 분산 알고리즘을 클라이언트 라이브러리로 구현하고 있는 특징
- key의 해시값을 서버대수로 나눈 나머지를 사용하는 단순한 버전 ~ Consistent Hashing과 같은 복잡한 것 까지 존재
- 사용측면에서 다수의 서버 중 1대가 다운되더라도 안전
- 서버 증감의 영향을 비교적 받지 않음
- 재시작 시, 원본 데이터가 사라지므로 원본 데이터 저장에는 부적합
- 재생성 시 시간이 걸리는 가공 데이터 저장에도 부적합
- 캐시 데이터가 가장 잘 활용됨
- RDBMS에서 읽어들인 데이터를 일시적으로 저장
- 참조할 때는 memcached를 먼저 참조해서 찾지 못한 경우에만 RDBMS를 참조
- 외부 리소스에 질의한 결과를 캐싱하는 등 다양한 캐시용 스토리지로 활용
- 서버에 충분한 메모리만 탑재하면 됨
- CPU나 I/O 성능은 그다지 요구되지 않음
#### TokyoTyrant
- TokyoCabinet에 네트워크를 지원하도록 구현
- 디스크에 데이터를 기록함으로써 데이터를 영속화할 수 있음
- 다중성을 높이기 위한 레플리케이션 기능을 내장
- 다양한 형식으로 데이터를 다루기 위한 API 마련
- 디스크 액세스가 발생하는 만큼 memcached보다는 성능이 떨어지지만 RDBMS와 비교하면 상당히 빠름
### 분산 파일시스템
- 분산 파일시스템도 스토리지의 유력 후보
- 파일시스템의 특성상 어느 정도 이상인 크기의 데이터를 저장하는데 적함
- NFS와 같이 그것이 고려되고 있는 구현을 제외하고 작은 데이터가 대량으로 존재하는 용도에는 적합하지 않음
#### MogileFS
- 비교적 작은 대량의 파일을 다룰 목적으로 구현된 분산 파일시스템
- RDBMS, 스토리지 서버, 그 사이를 연결하는 전송 서버로 구성
- 대량의 수KB~수십MB 정도의 이미지 파일을 효율적으로 저장하기 위한 시스템
- 갱신되지 않고 참조하기만 하는 용도에 적합
- 업로드 이미지 파일을 접수받는 웹 애플리케이션에 적합
- 스토리지 서버 상에서 개개의 파일은 실제 파일시스템 상에서도 하나의 파일로 저장
- 파일은 3중으로 다중화되어 일부 스토리지 서버가 고장나서 데이터가 손실되더라도 시스템 전체로서는 계속 정상적으로 동작하도록 설계
- 파일 저장장소와 파일을 특정 짓기 위한 키의 대응관계는 메타데이터로 RDBMS에 저장
- 파일 참조 시 마운트하는 것이 아니라 WebDAV 프로토콜로 얻게 됨
- 애플리케이션측의 구현이 필요
### 그 밖의 스토리지
#### NFS 계열 분산 파일시스템
- NFS는 특정 서버의 파일시스템을 다른 서버에서 마운트해서 해당 서버의 로컬 파일시스템과 마찬가지로 조작
- UNIX 시스템에 구현되어 있음
- 간단하게 사용 가능
- 커널 레벨에서 구현되어 있는 경우가 많아서 서버 측에 장애가 발생하면 클라이언트의 동작도 덩달아서 정지
- NFS의 개선 버전으로는 GlusterFS나 Lustre 등이 있음
    - 비교적 큰 파일을 다룰 경우 양호한 성능
- 어느정도 크기가 있는 데이터의 경우 NFS 등을 이용해서 파일시스템상에 직접 데이터를 저장하는 것이 비교적 현실적
#### WebDAV 서버
- HTTP를 기반으로 한 프로토콜
- 애플리케이션 계층에서 구현되는 경우가 많아서 보다 안정된 시스템 구축 가능
#### DRBD
- 네트워크 계층에서의 RAID라고 할 수 있음
- 블록 디바이스 레벨에서 분산, 다중화할 수 있는 기술
- 2대의 스토리지 서버의 블록 디바이스 간 동기를 실현
- 블록 디바이스 레벨에서의 분산 다중화는 RAID-1을 네트워크 상에서 실현
- 한쪽의 블록 디바이스 레벨에서의 완전한 복제를 다른 한쪽으로 유지
- 한쪽 서버에 장애가 발생한 경우는 장애 원인을 제거한 후 정상 데이터를 다시 동기화 함으로써 원래대로 복구
#### HDFS
- Hadoop용으로 설계된 분산 파일시스템
- 파일을 64MB씩 분할해서 저장
- 수백MB~수십GB의 거대한 데이터를 저장하는 것을 목적
- 기본적인 액세스는 Java API 경유
- MapReduce를 대상으로 한다는 특성
- 개개의 조작에 대한 응답이 빠르지 않으므로 실시간성 용도에는 적합하지 않음
### 스토리지 선택전략
- 책의 플로우 차트를 따르면 적절한 스토리지 선택 가능
- 해당 스토리지를 적절한 하드웨어 상에 구축하고 적절하게 설정, 튜닝할 필요가 있음
- 데이터 증가, 액세스 패턴변화에 따라 설정 변경 및 하드웨어 보강, 다른 스토리지로 옮김
## 3. 캐시 시스템
### 웹 애플리케이션의 부하와 프록시/캐시 시스템
- 웹 애플리케이션의 부하가 서서히 증가해서 시스템 용량이 부족해졌을 때에는 AP 서버나 DB 서버를 증설함으로써 대응할 수도 있지만,
- HTTP 레벨의 캐싱을 수행하는 HTTP 가속기를 사용함으로써 낮은 비용으로 효과가 높은 대책을 세울 수 있음
- HTTP 엑세스를 고속화하는 HTTP 가속기는 크게 포워드 프록시와 리버스 프록시 2종류가 있음
- 포워드 프록시는 클라이언트가 외부 서버에 액세스할 떄 사이에 두는 프록시
- 리버스 프록시는 외부의 클라이언트가 내부 서버에 액세스 할 때 사이에 두는 프록시
- 프록시에서는 요청에 대한 응답을 캐싱해둠으로써 다음에 같은 요청이 전달됐을 때 캐싱해둔 응답을 반환할 수 있음
- 이에 따라 대역이나 서버 리소스를 소비하지 않고 빠르게 요청을 처리할 수 있음
- 어느 정도 규모에 달한 웹 애플리케이션에서는 리버스 프록시를 이용한 캐시 서버를 효과적으로 이용
- 리소스 소비를 억제하면서 대량의 요청을 처리
- 갱신빈도가 낮은 동적인 페이지가 많을 경우에 유효함
#### 리버스 프록시 캐시 서버
- 리버스 프록시 캐시 서버의 구현으로 Squid가 가장 유명함
- Squid를 대체하기 위한 구현으로는 nginx, pound, Varnish 등이 개발되어있음
- Squid
    - HTTP, HTTPS, FTP용 다기능 프록시
    - 매우 강력한 캐시 기능을 갖춤
    - HTTP 대상 범용 캐시서버로 이용
    - 액세스 컨트롤이나 인증기능도 갖추고 있어서 높은 부하에 유연하게 견딜 수 있는 시스템을 위해 필수적인 툴
- Varnish
    - FreeBSD 개발자가 개발한 고성능 HTTP 가속기
    - 유연한 설정언어, 모던한 설게
    - 메모리 상에서 동작, Squid보다 빠르게 동작
### Squid
- 안정적으로 요청이 발생하고 있는 평상시의 효과
    - 일정 비율의 요청을 캐시 서버에서 반환하는 것을 기대
    - AP 서버로 전송되는 요청 수를 줄일 수 있음
    - AP 서버 대수의 증가를 억제, 감소할 수 있음
- 일부 콘텐츠에 비해 비정상적으로 요청이 발생하는 액세스 집중시의 효과 두 가지가 있음
    - 시스템 전체의 수용능력을 넘어서는 것을 막는 효과를 기대할 수 있음
    - 특정 콘텐츠로 집중되므로 효과적으로 캐싱할 수 있음
    - 캐시 서버가 캐싱된 것을 반환함으로써 액세스 집중 콘텐츠나 그 밖의 콘텐츠로의 액세스도 평소대로 반환할 수 있게됨
#### 여러 대의 서버로 분산하라
- Squid 서버를 2대 나열함으로써 다중성을 띄게할 수 있음
- 1대를 스탠바이로 남겨두거나 각각을 독립된 캐시 서버로 동작시키는 등 몇 가지 설정이 가능
- 2대의 Squid를 연계시키는 것은 ICP를 사용하는 것이 기본
- ICP는 Internet-Draft로 정의되어 있는 프로토콜의 일종, 캐시를 제어하기 위한 프로토콜
- 한쪽 캐시서버가 수신한 요청에 응답이 캐싱되지 않은 경우, 반대편 캐시 서버가 콘텐츠를 보유하고 있지 않은지 질의할 수 있음
- 두 캐시 서버 모두 보유하고 있지 않은 경우에만 부모 서버인 AP 서버로 질의
#### 2단 구성 캐시 서버
- CARP(Cache Array Routing Protocol)로 확장
- 이미지 파일 등 크기가 큰 파일을 캐싱하게 되면서 캐시 서버의 부하가 높아지면 1대나 2대정도로는 턱없이 부족할 경우가 있음
- 이런 경우 캐시 서버를 2단으로 구성함으로써 확장성이 높은 캐시 서버군을 구성할 수 있음
- Squid 프록시는 요청을 받아서 자신은 캐시를 보유하지 않고 하단 Squid 캐시 서버로 요청을 전송
- CARP 프로토콜에 따라 URL을 키로 적절한 Squid 캐시 서버로 전송
- URL을 키로 해서 하단 캐시서버를 선택함으로써 특정 URL에 대해 특정 캐시 서버만 사용
- 캐시 서버 대수가 늘어난 경우에도 효율적으로 캐싱할 수 있음
- 캐시 대상 URL수가 증가하더라도 하단 캐시 서버 대수만 늘려주면 부드럽게 확장
#### COSS 크기 결정방법
- 히트율을 높이기 위해서는 충분한 캐시 용량을 준비해둘 필요가 있음
- 캐시 용량은 크면 클수록 좋은 것은 아니고, 과부족이 없는 상태가 최적
- 캐시 용량이 너무 크면 단점
    - 초기 시작 시 COSS 파일 생성에 시간이 걸림
    - 서버 재시작 등으로 메모리가 초기화된 후에 디스크 상의 파일이 메모리에 올라가고 Squid의 성능이 안정되기까지 시간이 걸림
    - 디스크 용량을 압박함
- 반대로 너무 작으면 필요한 오브젝트가 저장되지 못해서 캐시 히트율이 떨어짐
- 최적의 용량은 1초당 저장된 오브젝트수 * 오브젝트의 평균 크기 * 오브젝트의 평균 유효시간
#### 투입 시 주의점
- Squid의 효율을 올리면 올리수록 필연적으로 Squid에 장애가 발생했을 때의 영향이 커짐
- 자주 있는 패턴으로, 2대로 부하를 분산하고 있는데 1대가 고장나고 남은 1대로는 부하를 감당할 수 없는 경우가 있음
- 1대가 고장 나더라도 문제가 없을 정도의 서버를 준비하는 것이 정석
- 새로운 서버를 추가할 때 무심하게 추가하면 성능이 단번에 떨어지는 경우도 있음
    - 재시작되거나 새로 구성한 Squid 서버가 요청을 처리하기 위한 준비가 되어 있지 않기 때문
    - 사전에 평상시에 접수되는 요청을 보내서 워밍업
### Varnish
- 리버스 프록시로서 캐시 서버에 특화된 구현, 모던한 아키텍처를 채택함으로써 Squid보다 높은 성능 확보
- Varnish는 고속화를 극한까지 추구한 설계, 다음 세가지는 주의
    - 오브젝트는 mmap에 의해 디스크 상의 파일에 저장됨, 프로세스를 재시작하면 캐시는 모두 사라짐
    - 기본적인 설정은 명령줄 옵션으로 주고 프록시로서의 규칙은 설정파일에 기술
    - 그 자체로는 로그를 파일에 기록하는 기능이 없고, 공유메모리 상에 기록
- 재시작 후에는 캐시 히트율이 0이 되므로 Squid 이상으로 주의깊게 재시작, 투입하는 것이 중요

## 4. 계산 클러스터
### 대량 로그 데이터의 병렬처리
- 대규모 웹 서비스를 운영하다 보면 로그 데이터도 대량으로 쌓임
- 한 번에 읽어들이는 것도 어렵고, 통계처리나 분석을 하려고 하면 엄청 큰 계산 리소스를 필요로 함
- 빠르게 수행하기 위해서는 병렬처리가 가능한 계산 클러스터가 필요
### MapReduce의 계산모델
- Hadoop이라는 MapReduce의 오픈소스 구현 사용
- MapReduce는 거대한 데이터를 빠르게 병렬로 처리하는 것을 목적으로 함
- 다수의 계산 노드로 구성된 클러스터와 대량 데이터를 분산해서 저장하기 위한 분산 파일시스템으로 구성
- MapReduce 계산 모델은 key와 value 쌍의 리스트를 입력 데이터로 해서 최종적으로 value의 리스트를 출력
- 계산은 기본적으로 Map 단계와 Reduce 단계로 구성
- Map 단계
    - 마스터 노드에서 입력 데이터를 잘게 분할해서 각 노드로 분산
    - 각 노드에서는 분할된 입력 데이터를 계산
    - 계산 결과를 key와 value쌍으로 구성된 중간 데이터로 출력
    - (k1, v1) -> list(k2, v2)
- Reduce 단계
    - Map 단계에서의 출력 데이터를 key(k2)별로 정리해서 key(k2)와 key에 대응하는 값의 리스트(list(v2))로 재구성
    - 각각의 key를 각 노드로 분산 (Shuffle Phase)
    - 각 노드에 있는 key(k2)와 key에 대응하는 값의 리스트리스트(list(v2))를 입력 데이터로 해서 리스트(list(v3))를 최종적인 출력 데이터로 하는 처리 수행
    - (k2, list(v2)) -> (k2, list(v3))
    - 각 노드에서 값의 리스트(list(v3))를 집약하면 계산 완료
- Map과 Reduce라는 두 가지 처리를 수행하는 함수를 준비하는 것만을호 대량의 데이터를 빠르게 처리할 수 있음
- 로그 분석, 검색엔진의 인덱스 생성 등 응용범위 광범위
- 입력 데이터를 읽어들이는 부분이 성능의 병목이 되는 경우가 많음
    - 분산 파일시스템과 병용하는 것이 중여
### Hadoop
- Apache 프로젝트 중 하나로 MapReduce의 오픈소스 구현 중 하나
