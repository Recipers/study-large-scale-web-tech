# 3. OS 캐시와 분산
- 대규모 데이터를 효율적으로 처리하는 원리
- 대규모 데이터를 다룰 때의 포인트
    - I/O 대책에 대한 기반은 OS에 있다
## 3.8 OS의 캐시 구조
### OS의 캐시 구조를 알고 애플리케이션 작성하기
- OS 캐시는 메모리를 이용하여 디스크 내의 데이터에 빠르게 엑세스 할 수 있도록 함
    - 페이지 캐시
    - 파일 캐시 (적절하지 않은 단어)
    - 버퍼 캐시
#### Linux(x86)의 페이징 구조를 예로
- 페이지
- 가상 메모리
    - OS는 가상 메로리 구조를 갖추고 있음
    - 논리적인 선형 어드레스를 물리 어드레스로 변환하는 것
### 가상 메모리 구조
- 물리적인 하드웨어를 OS에서 추상화 하기 위해 존재
- 그림 요약
    - 프로세스에서 메로리를 필요로 함
    - OS는 메모리에서 비어있는 곳을 찾음
    - 메모리의 비어 있는 곳의 주소를 반환
- 프로세스에서는 메모리의 어느 부분을 사용하는지 관여하지 않고, 정해진 규칙이 있으면 다루기 쉬움
- OS는 메모리를 직접 프로세스로 넘기는 것이 아니라 커널 내에서 메모리를 추상화 하고 있음
- 4KB 정도를 블록으로 확보해서 프로세스에 넘김
- 1개의 블록이 **페이지**
- OS는 프로세스에서 메모리를 요청받으면 페이지를 1개 이상, 필요한 만큼 넘김
### Linux의 페이지 캐시 원리
- OS는 확보한 페이지를 메모리상에 계속 확보해두는 기능을 갖고 있음
- 프로세스가 디스크로부터 데이터를 읽어내는 과정
    - 디스크로부터 4KB의 크기의 블록을 읽어냄
    - 한 번은 메모리상에 위치시켜야 함 (프로세스가 직접 디스크에 접근 불가)
    - OS는 읽어낸 블록을 메모리에 씀
    - 메모리 주소를 프로세스에 알려줌
    - 프로세스는 해당 메모리에 엑세스 하게됨
    - 데이터 읽기를 마친 프로세스가 처리가 완료되어 불필요 하게 되었어도 해제를 하지 않고 남겨둠
    - 다른 프로세스가 같은 디스크에 엑세스 할때에 남겨둔 페이지를 사용할 수 있음
        - 디스크를 읽으러 갈 필요가 없어짐 (페이지 캐시)
##### 페이지 캐시의 친숙한 효과
- 위 과정은 예외의 경우를 제외하고 모든 I/O에 작용함
- 디스크에서 데이터를 읽으러 가면 꼭 한번은 메모리로 가서 데이터가 반드시 캐싱됨
- 현대 OS는 대체로 페이지 캐시와 비슷한 구조를 갖추고 있음
- OS를 계속 가동시켜 두면 빨라짐
### VFS
- 디스크의 캐시는 페이지 캐시에 의해 제공됨
- 실제는 디바이스 드라이버와 OS 사이에 파일시스템이 끼어있음
- 파일시스템 하위에 디바이스 드라이버가 있고, 디바이스 드라이버가 하드디스크 등을 조작
- 파일 시스템 위에는 VFS(Virtual File System)라는 추상화 레이어가 있음
- VFS
    - 파일시스템의 다양한 함수의 인터페이스를 통일
    - 페이지 캐시의 구조를 지니고 있음
### Linux는 페이지 단위로 디스크를 캐싱한다
- 파일 캐시라는 단어가 부적절한 이유
- 디스크상의 큰 파일을 메모리상에 그 이미지를 캐싱할 수 없다고 생각할 수 있지만 그렇진 않음
- 4KB 블록을 캐싱, 특정 파일의 일부분, 읽어낸 부분만을 캐싱함
#### LRU
- Least Recently Used, 가장 오래된 것을 파기하고 가장 새로운 것을 남겨놓음
- DB도 계속 구동시키면 캐시가 점점 최적화 되어 갈수록 부하, I/O가 내려가는 특성
#### 어떻게 캐싱될까
- Linux는 파일을 i노드 번호로 식별함
- i노드 번호와 해당 파일의 어느 위치부터 시작할지를 나타내는 오프셋, 이 두가지 값을 키로 캐싱을 함
- OS 내부에서 사용되는 구조는 Radix Tree
- 파일이 아무리 커지더라도 캐시 탐색속도가 떨어지지 않도록 개발됨
### 메모리가 비어 있으면 캐싱
- Linux는 메모리가 비어 있으면 전부 캐싱함
- 메모리가 남아있지 않다면 오래된 캐시를 버리고 프로세스에 메모리를 확보해줌
- sar 명령어로 확인 가능
    - %kbcached : kilo byte cached, 캐시되어 있는 용량
    - %memused : 메모리 사용량
### 메모리를 늘려서 I/O 부하 줄이기
- 메모리를 늘리면 I/O 부하를 줄일 수 있음
    - 메모리를 늘리면 캐시에 사용할 수 있는 용량이 늘어남
    - 캐시 용량이 늘어나면 보다 많은 데이터를 캐싱할 수 있음
    - 디스크를 읽는 횟수가 줄어듬
### 페이지 캐시는 투과적으로 작용한다
- 매우 큰 파일을 읽은 만큼 메모리 사용량이 늘어난 모습
### sar 명령으로 OS가 보고하는 각종 지표 참조하기
- sar의 두 가지 사용법
    - 과거의 통계 데이터로 거슬러 올라가 접근
        - f 옵션으로 /var/log/sa 디렉터리에 저장된 로그파일을 지정
    - 현재의 데이터를 주기적으로 확인
        - sar 1 3 과같이 숫자 데이터 매개변수 지정 시 1초간격으로 3회 CPU 사용률 확인 가능
### sar -u -> CPU 사용률 확인
- 디폴트로 출력되는 사용률 정보는 sar -u
    - user : 사용자 모드에서 CPU가 소비된 시간의 비율
    - nice : nice로 스케쥴링의 우선도를 변경한 프로세스가 사용자 모드에서 CPU를 소비한 시간의 비율
    - system : 시스템 모드에서 CPU가 소비된 시간의 비율
    - iowait : CPU가 디스크 I/O 대기를 위해 Idle 상태로 소비한 시간의 비율
    - steal : Xen 등 OS 가상화를 이용하고 있을 경우, 다른 가상 CPU의 계산으로 대기된 시간의 비율
    - idle : CPU가 디스크 I/O 대기 등으로 대기되지 않고, Idle 상태로 소비한 시간의 비율
### sar -q -> Load Average 확인
- 실행 큐에 쌓여 있는 프로세스의 수, 시스템상의 프로세스 사이즈 Load Average 등을 참조할 수 있음
### sar -r -> 메모리 사용현황 확인
- 물리 메모리의 이용 상황 파악 가능
    - kbmemfree : 물리 메모리의 남은 용량
    - kbmemused : 사용 중인 물리 메모리 양
    - memused : 물리 메모리의 남은 용량
    - kbbuffers : 커널 내의 버퍼로 사용되고 있는 물리 메모리의 용량
    - kbcached : 커널 내에서 캐시용 메모리로 사용되고 있는 물리 메모리의 용량
    - kbswpfree : 스왑 영역 남은 용량
    - kbswpused : 사용 중인 스왑의 용량
### sar -W -> 스왑 발생상황 확인
- 스왑 발생 상황 확인 가능
    - pswpin/s : 1초 동안 스왑인되고 있는 페이지 수
    - pswpout/s : 1초동안 스왑아웃되고 있는 페이지 수
## 3-9. I/O 부하를 줄이는 방법
### 캐시를 전제로 한 I/O 줄이는 방법
- 캐시에 의한 I/O 경감효과는 매우 크고 기본임
    - 데이터 규모에 비해 물리 메모리가 크면 전부 캐싱할 수 있음
        - 대규모 데이터 처리에는 압축이 중요
        - 압축해서 저장해두면 디스크 내용 전부 그대로 캐싱해둘 수 있는 경우가 많음
    - 경제적인 비용과의 밸런스를 고려하고자 함
### 복수 서버로 확장시키기
- 캐시로 해결될 수 없는 규모일 경우
- DB 서버를 늘려야 할 때는 반드시 부하 떄문만은 아니고 캐시 용량을 늘리고자 할 때, 효율을 높이고자 할 때
- DB 서버는 늘리면 좋다라는 논리가 맞지 않음
### 단순히 대수만 늘려서는 확장성을 확보할 수 없다
- 단순히 데이터를 복사해서 대수를 늘리게 되면, 그 부족한 부분 그대로 동일하게 늘려가게 되는 것
- 시스템 전체적으로는 조금 빨라질 수는 있지만 비용대비 성능향상이 극히 부족함
### I/O 부하 줄이기와 페이지 캐시
- Linux는 가능한 남아있는 메모리를 페이지 캐시로 활용하려고 한다
    - 디스크로부터 데이터를 읽어서
    - 아직 그것이 페이지 캐시에 없고
    - 메모리가 남아있다면
    - 언제든 새로운 캐시를 생성한다
- 캐시용 메모리가 없다면 오래된 캐시를 버리고 새로운 캐시로 교체
### 페이지 캐시에 의한 I/O 부하의 경감효과
- 메모리 증설이 I/O 부하 경감에 효과적인 방법임
- 메모리를 증설할 수 없을 경우에는 데이터를 분할해서 각각의 서버에 위치시키는 것을 검토
### 페이지 캐시는 한 번의  read에서 시작된다
- 캐싱하지 못한 데이터는 직접 디스크에서 읽어들임
- 부팅 직후에는 거의 모든 읽기 요청은 디스크로부터 전송됨
    - DB 서버 재부팅시 조심해야 할 사항
## 3-10. 국소성을 살리는 분산
### 국소성을 살리는 분산이란?
- 캐시 용량을 늘리기 위해 어떻게 하면 여러 대의 서버로 확장시키기 위해 국소성을 고려함
- 국소성 (locality)
    - 데이터에 대한 엑세스 패턴을 고려해서 분산시키는 것
    - 데이터 액세스 패턴을 고려하지 않는 경우, 데이터 영역을 전부 캐싱해야 되는 문제 발생
### 파티셔닝
- 국소성을 고려한 분산
- 한 대였던 DB 서버를 여러 대의 서버로 분할하는 방법
- 간단한 것은 테이블 단위의 분할
    - 테이블 단위로 각기 다른 서버로 분할
- 테이블 데이터 분할
    - 하나의 테이블을 여러개의 작은 테이블로 분할
    - id의 첫글자 별 등의 규칙으로 다른 db서버에 분할하여 저장
### 요청 패턴을 '섬'으로 분할
- 용도별로 시스템을 섬으로 나누는 방법
    - 요청의 성질(user-agent, url 등)을 기준으로 테이블을 분할
### 페이지 캐시를 고려한 운용의 기본 규칙
- OS 기동 직후에 서버를 투입하지 않음
    - 캐시가 쌓여 있지 않기 떄문
    - 자주 사용하는 DB의 파일을 한 번 cat 해줌
- 성능평가, 부하테스트 시 초깃값을 버려야 함
    - 캐시 최적화가 이루어 진 후 실시
### 부하분산과 OS의 동작 원리
- OS 동작원리를 알아야 부하분산 학습에 도움이 됨
- OS의 동작 원리
    - OS 캐시
    - 멀티스레드 or 멀티 프로세스
    - 가상 메모리 구조
    - 파일시스템
