# 10장 - (과제) 전문 검색엔진 작성
- 이번 장에서는 실제 과제를 통해 검색엔진을 구현해본다.
	- Dictionary + Posting 쌍으로 구성된 역 인덱스를 구현
- 전문 검색엔진 작성
	- [과제] 하테나 북마크 전문 검색 만들기 - 강의 27
	- 응답 사례와 사고방식 - 강의28

- - -
## 강의27: [과제] 하테나 북마크 전문 검색 만들기
### 전문 검색엔진 개발
- 필수
	- 하테나의 최근 엔트리 1만건 대상
	- 검색어를 포함하는 엔트리 반환
	- 반환 내용으로 `URL`, `타이틀`을 포함해야 됨
	- 가능하면 스니핏도 표시
	- `작성일자순`으로 정렬
- 추가
	- 검색 조건 추가 : AND / OR 검색 대응, 카테고리 분류 대응
	- 속도와 정확성 추구 : 실용적인 검색속도, 검색누락이나 `false-positive` 회피
		- **false-positive** : 거짓양성, 긍정 오류(정상이 아닌데 정상으로 판단하는 경우)

### 요약
- 최근 1만 건의 데이터 엔트리로 검색어를 포함하는 엔트리를 반환하는 프로그램 만들기
- `타이틀`이나 `URL`뿐 아니라 본문에 `검색어`가 포함되어 있는지 여부도 체크 후 해당하는 엔트리 반환하기
- 검색 결과로는 엔트리의 URL과 타이틀 반환 -> 추가적으로 여유 있다면 스니핏도 출력해보기
- 검색결과 정렬은 최신 작성일자 순으로 정렬 -> 가능하다면 스코어링으로?

#### 과제를 해결하면 얻는 이점
- RDB의 한계를 돌파해보는 경험

### 샘플 데이터 형식과 데이터 크기
1. URL이나 타이틀 등 엔트리의 기본 데이터가 10,000건 기록되어 있는 텍스트
2. 각 엔트리의 본문 텍스트 파일 10,000개
```plain text
15283314    4    https://www.~~~    제목1
15283311    3    https://www.~~~    제목2
            .
            .
```
- `엔트리ID | 카테고리ID | URL | 타이틀` 의 형태로 되어 있음
- 10,000 건에 1.4MB 정도, 본문 데이터는 10,000 건에 55MB
	- 실제 하테나 북마크 전체를 대상으로 하면 3,000만 건을 넘음 (기가바이트 단위)

### 사전의 구성 - Dictionary, Postings
- Dictionary는 단어를 기반으로 만듦 (n-gram X)
	- 본문 데이터에 대한 term은 AC법이든, MeCab을 사용하던 상관 없음
- Postings의 압충은 6장 과제에서 만든 `VB Code`를 참고해 만들자

### 애플리케이션 인터페이스
- UI에서 포함되어야 할 요소
	- 명령줄에서 실행
	- 대화형 인터페이스
- UNIX 명령줄(쉘)에서 search.pl 같이 명령을 입력하면 검색어를 받아들일 수 있는 상태가 되고, `하테나`라고 입력하면 `하테나 다이어리 ... URL`과 같이 검색 결과를 반환해주는 형태 -> 페이징은 원하는 대로

### 기본적인 부분 + 심화 구현
- 위 단계까지 완료되면 기본적인 구현은 완료되었다고 다음의 심화 단계를 고려해보자.
	- 전문 검색 구현
	- `AND` / `OR` 검색 지원
	- 카테고리에 따라 분류

### 속도와 정확성으로 승부
- 여유가 있다면 다음을 고려해보자.
	- 검색 시간: 100개의 샘플 쿼리를 날렸을 때 모든 검색결과를 반환하는 데 걸리는 시간
	- 정확성(검색누락, false-positive): 검색 결과 상위 5건에 대해 `검색 누락`과 `정렬`이 올바른지

- - -
## 강의28: 응답 사례와 사고방식
### 응답 사례
- `indexer.pl`과 `searcher.pl` 두 개의 스크립트 파일로 설명
	- `searcher.pl`은 스니핏도 출력, 검색은 쿼리 한 단어로만 검색 (AND / OR: X)

### indexer.pl 구현
- 주어진 데이터를 읽고 이로 역 인덱스를 만듦
	- Dictionary는 Text::MeCab 사용 (MeCab, 형태소 기반 사전)
- 역 인덱스는 Perl의 해시로 만듦
	- Postings List의 압축은 차분을 구해 VB Code 적용
	- 라이브러리를 사용하고 싶다면 Array::GAp 라이브러리 사용
- 역 인덱스가 구축되면 Perl의 Storable이라는 Serialize / Deserialize 라이브러리 사용
	- 이를 통해 디스크에 해시 기록
	- `Storable`: 해시 데이터 구조를 바이너리로 디스크에 기록, 다른 프로그램에서 로드 가능

### searcher.pl 구현
- 입력으로 주어진 데이터 파일을 읽어드리고, indexer.pl로 만든 인덱스 파일을 로드
	- 로드된 역 인덱스는 그대로 Perl의 해시로 사용
- 검색용 인터페이스는 대화 프롬프트로서 제공 -> 이를 위해 Term::ReadLine 사용
- 스니핏 출력은 간단한 과제 상황이므로 바로 텍스트 내에서 쿼리가 일치하는 곳을 찾도록 구현
	- 실용적인 시스템에서는 검색어의 출현 위치를 기억시켜 두고 쿼리를 발견한 위치는 곧바로 찾지말고 속도를 버는 편이 좋음

### 개선할 수 있는 점은?
- AND / OR 검색 구현
	- `AND`: 복수의 Postings List 사이에서 문서 ID의 교집합을 얻는다.
	- `OR`: 합집합을 얻는다.
- searcher.pl로 검색어도 분해한다
	- AND / OR 검색을 지원하지 않으므로 검색어를 그대로 역 인덱스에 집어넣고 있음
	- 실제 쿼리 단어도 형태소 분석한 후 역 인덱스로 검색하는 편이 정확도는 더 향상됨
- 시도해 볼만한
	- 형태소 분석이 아닌 n-gram으로 변경
	- 역 인덱스에 단어 출현 위치를 기록해 스니핏을 출력하는 데 도움을 받도록

### Column - Twitter의 스케일아웃 전략
- 이때 당시의 배경에서 트위터도 MySQL + memcached + 파티셔닝으로 인프라 구성
- 트위터는 사용자 ID를 이용한 파티셔닝이 아니라 트윗 작성일시를 축으로 한 파티셔닝 사용
- fan out이라고 하는 메일 전송과 비슷한 아키텍처를 MySQL + memcached로 실시간성 지원
	- Apache Cassandra로 변경할 수도 있음

